# Accelerator training configuration (optimized)
async_upload: true
bucket_no_upscale: true
bucket_reso_steps: 64
enable_bucket: true

cache_latents: true
cache_latents_to_disk: true
caption_extension: ".txt"
clip_skip: 1

dynamo_backend: "no"
epoch: 20                             # ↗️ doubled epochs for more training passes
gradient_accumulation_steps: 4       # ↗️ effective batch size ×4
gradient_checkpointing: true

huber_c: 0.1
huber_schedule: "snr"

# HF repo settings
huggingface_path_in_repo: "checkpoint"
huggingface_repo_id: ""
huggingface_repo_type: "model"
huggingface_repo_visibility: "public"
huggingface_token: ""

# Learning rate & scheduler
learning_rate: 1e-5
lr_scheduler: "cosine"
lr_scheduler_args:
  - num_warmup_steps=200             # ↗️ warmup for stability
lr_scheduler_num_cycles: 1
lr_scheduler_power: 1

loss_type: "l2"
max_grad_norm: 1

# Resolution bucketing
max_bucket_reso: 2048
min_bucket_reso: 256

# Data loading
max_data_loader_n_workers: 8         # ↗️ parallel data loading

# Training steps & precision
max_timestep: 1000
max_token_length: 75
max_train_steps: 1600

min_snr_gamma: 5
mixed_precision: "bf16"

# LoRA network
network_module: "networks.lora"
network_alpha: 64                    # ↗️ stronger LoRA scaling
network_dim: 64                      # ↗️ wider LoRA layers
network_args:
  - train_double_block_indices=all
  - train_single_block_indices=all

no_half_vae: true
noise_offset_type: "Original"

# Optimizer
optimizer_type: "AdamW8bit"
optimizer_args:
  - weight_decay=0.01
  - betas=(0.9,0.999)
  - eps=1e-8
  - warmup_init=True

# Output & checkpoints
output_dir: "/app/outputs"
output_name: "last"
save_every_n_epochs: 5               # ↗️ more frequent saves
save_model_as: "safetensors"
save_precision: "bf16"

# Pretrained & prior
pretrained_model_name_or_path: "stabilityai/stable-diffusion-xl-base-1.0"
prior_loss_weight: 1

resolution: "1024,1024"
sample_prompts: ""
sample_sampler: "euler_a"

# Learning rates per component
text_encoder_lr: 1e-5
unet_lr: 1e-5

train_batch_size: 4                  # per‑GPU batch (×4 accumulation → 16)
train_data_dir: ""
training_comment: ""

# Memory & acceleration
scale_weight_norms: 5
xformers: true                       # enable memory-efficient attention
