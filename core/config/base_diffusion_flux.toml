# Accelerator training configuration
ae: "/app/flux/ae.safetensors"                        
apply_t5_attn_mask: true                              
bucket_no_upscale: true                               
bucket_reso_steps: 64                                 
cache_latents: true                                   
cache_latents_to_disk: true                           
caption_extension: ".txt"                             
clip_l: "/app/flux/clip_l.safetensors"                
discrete_flow_shift: 3.1582                           
dynamo_backend: "no"                                  
epoch: 100                                            
full_bf16: true                                       

# Increase effective batch size via accumulation
gradient_accumulation_steps: 4                        
gradient_checkpointing: true                          
guidance_scale: 1.0                                   
highvram: true                                        
huber_c: 0.1                                          
huber_scale: 1                                       
huber_schedule: "snr"                                 

# Repository settings
huggingface_path_in_repo: "checkpoint"                
huggingface_repo_id: ""                               
huggingface_repo_type: "model"                        
huggingface_repo_visibility: "public"                 
huggingface_token: ""                                 

# Loss & scheduler
loss_type: "l2"                                       
lr_scheduler: "cosine"                                
lr_scheduler_args:
  - num_warmup_steps=500
lr_scheduler_num_cycles: 1                            
lr_scheduler_power: 1                                 

# Resolution bucketing
max_bucket_reso: 2048                                 
max_data_loader_n_workers: 8        # ↗️ parallel data loading
max_timestep: 1000                                    
max_train_steps: 3000                                 
mem_eff_save: true                                    
min_bucket_reso: 256                                  

# Precision & prediction
mixed_precision: "bf16"                               
model_prediction_type: "raw"                          

# LoRA network
network_alpha: 128                                    
network_args:
  - train_double_block_indices=all
  - train_single_block_indices=all
  - train_t5xxl=True
network_dim: 128                                      
network_module: "networks.lora_flux"                  

noise_offset_type: "Original"                         

# Optimizer
optimizer_type: "AdamW"          # more stable than Adafactor
optimizer_args:
  - lr=5e-5                     # base LR
  - weight_decay=0.01
  - betas=(0.9,0.999)
  - eps=1e-8
  - warmup_init=True            # enable warmup bias correction

output_dir: "/app/outputs"                             
output_name: "last"                                    
pretrained_model_name_or_path: "/app/flux/unet.safetensors"  
prior_loss_weight: 1                                   

# Training & sampling
resolution: "1024,1024"                                
sample_prompts: ""                                     
sample_sampler: "euler_a"                              
save_every_n_epochs: 25                                
save_model_as: "safetensors"                           
save_precision: "float"                                
seed: 1                                                

# Text encoder & T5
t5xxl: "/app/flux/t5xxl_fp16.safetensors"              
t5xxl_max_token_length: 512                            
text_encoder_lr:
  - 5e-5
  - 5e-5

# Scheduler style
timestep_sampling: "sigmoid"                           

# Batch sizes
train_batch_size: 4            # ↗️ per‑GPU batch
vae_batch_size: 8              # larger VAE decode batch

# Model LRs
unet_lr: 5e-5                                          

# Logging & acceleration
wandb_run_name: "last"                                 
xformers: true                                          # enable memory‐efficient attention
